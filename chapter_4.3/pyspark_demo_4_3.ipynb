{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of paper \"Econometrics at Scale - Spark Up Big Data in Economics\", Chapter 4.3 (B. Bluhm and J. Cutura)\n",
    "This notebook implements a distributed panel data regression exercise in PySpark as described in more detail in the paper \"Econometrics at Scale - Spark Up Big Data in Economics\" by Benjamin Bluhm and Jannic Cutura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "Note: if you want to execute this notebook on your local PC you need to create a Spark session (not required on AWS EMR notebook instance).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to compute:\n",
    "- bread-and-meat matrix for panel robust standard errors\n",
    "- degrees of freedom adjusted standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bread_meat(iterator):\n",
    "    \"\"\"\n",
    "\n",
    "    :param iterator:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Cast data to list\n",
    "    data = list(iterator)\n",
    "    \n",
    "    # Construct arrays of features and residuals\n",
    "    X = np.array([i[1][2:] for i in data])\n",
    "    X_key = np.array([i[0] for i in data])\n",
    "    X_time = np.array([i[1][0] for i in data])\n",
    "    uhat = np.array([i[1][1] for i in data])\n",
    "\n",
    "    # Set paramaters\n",
    "    k = X.shape[1]\n",
    "    n = len(np.unique(X_key))\n",
    "\n",
    "    # Generate key and time lists\n",
    "    tind = [[] for i in range(n)]\n",
    "    tlab = [[] for i in range(n)]\n",
    "\n",
    "    for index, i in np.ndenumerate(np.unique(X_key)):\n",
    "        tind[index[0]] = np.where(X_key == i)\n",
    "\n",
    "    # Initialze bread and meat arrays\n",
    "    xtx = np.zeros((k, k, n))\n",
    "    xuux = np.zeros((k, k, n))\n",
    "\n",
    "    # Compute array cross-products\n",
    "    for i in range(n):\n",
    "        x = X[tind[i]]\n",
    "        xt = np.transpose(x)\n",
    "        u = uhat[tind[i]]\n",
    "        ut = np.transpose(u)\n",
    "        xtx[:, :, i] = xt.dot(x)\n",
    "        xuux[:, :, i] = np.matmul(np.array([xt.dot(u)]).T, np.array([ut.dot(x)]))\n",
    "\n",
    "    XtX = np.sum(xtx, axis=2)\n",
    "    XuuX = np.sum(xuux, axis=2)\n",
    "\n",
    "    yield [XtX, XuuX]\n",
    "\n",
    "    \n",
    "# Compute degrees of freedom adjusted standard errors\n",
    "def compute_se_df_adjusted(n, t, k, summary):\n",
    "    \"\"\"\n",
    "    :param n:\n",
    "    :param t:\n",
    "    :param k:\n",
    "    :param summary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    adjustment_factor = 1 / (n * (t - 1) - k) * (n * t - k)\n",
    "    se = [math.sqrt(adjustment_factor) * i for i in summary.coefficientStandardErrors]\n",
    "    return se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to parquet with simulated panel data\n",
    "path = 's3://my-bucket/data/panel'\n",
    "\n",
    "# specify independent variables\n",
    "independent_vars = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\"]\n",
    "\n",
    "# specify columns for panel ID, target variables and time dimension\n",
    "id_var = \"id\"\n",
    "dep_var = \"target\"\n",
    "time_var = \"time\"\n",
    "\n",
    "# define number of partitions\n",
    "num_partitions = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within-group transformation via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df = spark.sql(\"\"\" SELECT id, time, target, x1, x2, x3, x4, x5, x6, x7 FROM df\"\"\")\n",
    "\n",
    "# create dataframe for standard ols regression\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df_train = spark.sql(\"\"\"SELECT id, time, target, x1, x2, x3, x4, x5, x6, x7 FROM df\"\"\")\n",
    "df_train.createOrReplaceTempView(\"df_train\")\n",
    "\n",
    "# perform within-group transformation\n",
    "df_train_between = spark.sql(\"\"\"SELECT id, \n",
    "                             avg(target) as target, \n",
    "                             avg(x1) as x1, \n",
    "                             avg(x2) as x2, \n",
    "                             avg(x3) as x3, \n",
    "                             avg(x4) as x4, \n",
    "                             avg(x5) as x5, \n",
    "                             avg(x6) as x6, \n",
    "                             avg(x7) as x7 \n",
    "                             FROM df\n",
    "                             GROUP BY id\"\"\")\n",
    "df_train_between.createOrReplaceTempView(\"df_train_between\")\n",
    "\n",
    "df_train_within = spark.sql(\"\"\"SELECT id, time,\n",
    "                                 target + (select avg(target) from df) as target, \n",
    "                                 x1 + (select avg(x1) from df) as x1, \n",
    "                                 x2 + (select avg(x2) from df) as x2, \n",
    "                                 x3 + (select avg(x3) from df) as x3, \n",
    "                                 x4 + (select avg(x4) from df) as x4, \n",
    "                                 x5 + (select avg(x5) from df) as x5, \n",
    "                                 x6 + (select avg(x6) from df) as x6, \n",
    "                                 x7 + (select avg(x7) from df) as x7\n",
    "                                 FROM df\"\"\")\n",
    "df_train_within.createOrReplaceTempView(\"df_train_within\")\n",
    "\n",
    "df_train_within = spark.sql(\"\"\"SELECT a.id, a.time,\n",
    "                           (a.target - b.target) as target,\n",
    "                           (a.x1 - b.x1) as x1,\n",
    "                           (a.x2 - b.x2) as x2,\n",
    "                           (a.x3 - b.x3) as x3,\n",
    "                           (a.x4 - b.x4) as x4,\n",
    "                           (a.x5 - b.x5) as x5,\n",
    "                           (a.x6 - b.x6) as x6,\n",
    "                           (a.x7 - b.x7) as x7\n",
    "                           FROM df_train_within as a\n",
    "                           JOIN df_train_between as b ON a.id = b.id\"\"\") \n",
    "\n",
    "# apply VectorAssembler to prepare input data in required format \n",
    "vector_assembler = VectorAssembler(inputCols = independent_vars, outputCol = 'features')\n",
    "df_train = vector_assembler.transform(df_train)\n",
    "df_train_within = vector_assembler.transform(df_train_within)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit linear model \n",
    "- without data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "# Standard OLS\n",
    "model = GeneralizedLinearRegression(labelCol=\"target\", featuresCol=\"features\").fit(df_train)\n",
    "summary_ols = model.summary\n",
    "print(summary_ols)\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit linear model with \n",
    "- transformed data & degrees of freedom adjusted standard errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "# Fixed effects model\n",
    "model = GeneralizedLinearRegression(labelCol=\"target\", featuresCol=\"features\").fit(df_train_within)\n",
    "summary_panel = model.summary\n",
    "print(compute_se_df_adjusted(1000000000, 10, 8, summary_panel))\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute panel robust standard errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "# Predict, compute residuals and add intercept\n",
    "df_train_within = model.transform(df_train_within)\n",
    "df_train_within.createOrReplaceTempView(\"df\")\n",
    "df_train_within = spark.sql(\"\"\"SELECT df.*, (target - prediction) as u, 1 as intercept FROM df\"\"\")\n",
    "\n",
    "# Select relevant columns for computing sandwich VCE\n",
    "df = df_train_within.select([\"id\", \"time\", \"u\", \"intercept\"] + independent_vars)\n",
    "\n",
    "# Create hash partitioner assuring that data for each id is in one partition\n",
    "def key_partitioner(id):\n",
    "    return hash(id)\n",
    "\n",
    "# Create RDD with 1,000 partitions\n",
    "key_value_rdd = df.rdd.map(lambda x: (x[0], x[1:11])).partitionBy(1000, key_partitioner)\n",
    "\n",
    "# Compute array cross-products for sandwich VCE and collect results to master node\n",
    "arr_bread_meat = key_value_rdd.mapPartitions(compute_bread_meat).collect()\n",
    "\n",
    "# Construct bread and meat arrays and sandwich VCE\n",
    "bread = np.linalg.inv(sum([item[0] for item in arr_bread_meat]))\n",
    "meat = sum([item[1] for item in arr_bread_meat])\n",
    "vcov = bread.dot(meat).dot(bread)\n",
    "\n",
    "# Compute robust standard errors (excluding intercept)\n",
    "se = list(np.sqrt(vcov).diagonal())[1:]\n",
    "print(se)\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
